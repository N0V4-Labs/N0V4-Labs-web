<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="How PHALANX's memory system uses a 2 GB budget with frame, pool, and heap allocators to keep allocation costs predictable.">
  <title>Inside PHALANX's Memory System: 2 GB On Purpose</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600&family=Merriweather:ital,wght@0,300;0,400;0,700;0,900;1,400&family=Montserrat:wght@400;700;900&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="styles.css">
</head>
<body>
<header>
  <nav class="navbar">
    <div class="container">
      <div class="nav-brand">
        <h1><a href="index.html" style="color: inherit; text-decoration: none;">ziX Performance Labs</a></h1>
      </div>
      <ul class="nav-menu">
        <li><a href="about.html">About</a></li>
      </ul>
    </div>
  </nav>
</header>

<main>
  <div class="container">
    <section class="blog-content">
      <article class="blog-post">
        <h2>Inside PHALANX's Memory System: 2 GB On Purpose</h2>
        <div class="meta">
          October 25, 2025
        </div>
        <div class="content">
          <p>
            Here is something that happens very reliably once you try to run a 144&nbsp;FPS mech game for real:
          </p>
          <p>
            At the beginning, you lean on the default allocator. It seems fine. Memory graphs look noisy but
            tolerable, frame times are mostly OK, and the occasional spike is easy to blame on "debug mode" or
            "editor overhead."
          </p>
          <p>
            Then you turn on full gameplay: ECS, physics, particles, replay, UI, networking. The allocator
            becomes the hidden boss fight. Latency spikes are hard to reproduce. Fragmentation slowly increases
            over a long match. Profiling turns into an archaeology dig through <code>operator new</code>.
          </p>
          <p>
            PHALANX's memory system exists so that we do not have to live in that world.
          </p>
          <p>
            It is built around a small set of ideas:
          </p>
          <ul>
            <li>There is a hard budget (≈2&nbsp;GB) and a clear split between frame, pool, and heap usage.</li>
            <li>Allocations come from explicit, named allocators, not the global heap by accident.</li>
            <li>Every byte we care about shows up in Tracy as a timeline, a graph, or both.</li>
            <li>When we exceed a budget, that is a design problem, not a surprise.</li>
          </ul>
          <p>
            The style is the same as with the ECS layer: give ourselves rules first, then write code that has
            a chance of obeying them.
          </p>

          <h3>Numbers first</h3>
          <p>
            We write down an actual budget:
          </p>

          <div class="code-block">
<pre><code>Frame allocator (per-frame scratch):    16 MB
Pool allocators (entities, effects):   512 MB
General heap (assets, systems):       1536 MB
---------------------------------------------
Total runtime memory budget:          ~2048 MB</code></pre>
          </div>

          <p>
            The budget is not a promise about every platform on earth; it is an expectation for the machines
            we care about.
          </p>
          <p>
            From that, we get a set of operation-level targets in the same spirit as the ECS table:
          </p>
          <ul>
            <li>Frame allocation: &lt;&nbsp;50&nbsp;ns</li>
            <li>Pool allocation / free: &lt;&nbsp;60–80&nbsp;ns</li>
            <li>Typical heap allocation: &lt;&nbsp;100&nbsp;ns for small/medium sizes</li>
            <li>Frame reset: &lt;&nbsp;100&nbsp;ns</li>
          </ul>
          <p>
            If we drift far outside of these, something is wrong. The point is not that the numbers are magic;
            the point is that we can talk concretely about "this new system adds 0.3&nbsp;ms of allocator
            overhead" instead of guessing.
          </p>

          <h3>A small surface: the allocator interface</h3>
          <p>
            Just as we do not let gameplay code talk to Flecs directly, we do not let most code talk
            to the raw STL allocator either.
          </p>
          <p>
            Instead, there is a small allocator interface in <code>phalnx::memory</code>. The intent is simple:
          </p>
          <ul>
            <li>Systems accept an <code>IAllocator&amp;</code> (or a more specific allocator reference) instead of reaching for <code>new</code>.</li>
            <li>Frame, pool, and free-list allocators all conform to the same minimal API.</li>
            <li>If a particular strategy turns out to be wrong, we can swap it without changing every call site.</li>
          </ul>
          <p>
            The interface looks roughly like this at the conceptual level:
          </p>

          <div class="article-code draw-left-line">
            <p class="article-code-file mark-section">allocator.h</p>
            <pre class="article-code-text cpp">struct IAllocator {
  void* Allocate(std::size_t size, std::size_t alignment);
  void  Deallocate(void* ptr, std::size_t size, std::size_t alignment);
};</pre>
          </div>

          <p>
            Typed helpers sit on top:
          </p>

          <div class="article-code draw-left-line">
            <p class="article-code-file mark-section">allocator.h</p>
            <pre class="article-code-text cpp">template &lt;typename T&gt;
T* Allocate(IAllocator&amp; alloc) {
  return static_cast&lt;T*&gt;(alloc.Allocate(sizeof(T), alignof(T)));
}</pre>
          </div>

          <p>
            There is no exception in sight. If an allocation fails, you get <code>nullptr</code> and the
            surrounding system decides what to sacrifice. That is a deliberate pattern: error handling happens
            at the gameplay or engine level, not buried inside an allocator.
          </p>
          <p>
            The allocator interface is the memory analogue of <code>EntityManager</code>: a place to attach
            documentation, to enforce conventions, and to keep the rest of the code from depending directly
            on whatever allocation strategy we are using this month.
          </p>

          <h3>The frame allocator: memory that evaporates on purpose</h3>
          <p>
            A frame allocator is what you get when you admit that a large fraction of your allocations have
            lifetimes measured in milliseconds.
          </p>
          <p>
            The implementation is the standard bump-pointer pattern:
          </p>
          <ul>
            <li>One contiguous buffer (16&nbsp;MB by default).</li>
            <li>One offset.</li>
            <li><code>Allocate</code> advances the offset with alignment padding.</li>
            <li><code>Deallocate</code> is a no-op.</li>
            <li><code>Reset</code> sets the offset back to zero at the end of the frame.</li>
          </ul>
          <p>
            Allocation is a few arithmetic instructions. Reset is effectively free.
          </p>
          <p>
            The intended usage looks like this:
          </p>

          <div class="article-code draw-left-line">
            <p class="article-code-file mark-section">frame_allocator.cpp</p>
            <pre class="article-code-text cpp">void RenderFrame() {
  auto* cmds  = frame_alloc.AllocateArray&lt;DrawCommand&gt;(1000);
  auto* rays  = frame_alloc.AllocateArray&lt;RaycastHit&gt;(4096);
  auto* state = frame_alloc.Allocate&lt;TemporaryState&gt;();

  // ... use buffers for this frame only ...

  frame_alloc.Reset();  // all of the above is now invalid
}</pre>
          </div>

          <p>
            It is impossible to overemphasize the contract:
          </p>
          <ul>
            <li>Anything allocated from the frame allocator dies at <code>Reset()</code>.</li>
            <li>Storing those pointers into long-lived structures is undefined behavior.</li>
          </ul>
          <p>
            The value is that any system that can be expressed in this shape becomes vastly simpler:
          </p>
          <ul>
            <li>Physics can build temporary contact graphs, constraint lists, and query results without worrying about frees.</li>
            <li>Rendering can emit command buffers and transient upload blobs without touching the general heap.</li>
            <li>ECS can build temporary views or job queues for the next phase without fragmenting anything.</li>
          </ul>
          <p>
            The allocator is fast because it is allowed to be ruthless about lifetime.
          </p>

          <h3>Pool allocators: bullets, minions, particles</h3>
          <p>
            Not all lifetimes are one frame. Projectiles, minions, particles, and various kinds of gameplay
            state live for seconds or minutes, but they share a crucial property: they are all the same size.
          </p>
          <p>
            For that pattern, we use pool allocators.
          </p>
          <p>
            Conceptually, each pool is:
          </p>
          <ul>
            <li>A fixed-size array of slots, each big enough to hold one <code>T</code>.</li>
            <li>An intrusive free-list stored in the slots themselves when they are free.</li>
            <li>A simple, O(1) <code>Allocate</code>/<code>Deallocate</code> pair that manipulates the free-list head.</li>
          </ul>
          <p>
            The public API looks like a thin layer over that:
          </p>

          <div class="article-code draw-left-line">
            <p class="article-code-file mark-section">pool_allocator.cpp</p>
            <pre class="article-code-text cpp">PoolAllocator&lt;Projectile&gt; projectile_pool(/* capacity */, "ProjectilePool");

Projectile* p = projectile_pool.Construct(args...);
if (!p) {
  // pool exhausted; decide what to drop
}

projectile_pool.Destroy(p);</pre>
          </div>

          <p>
            Two things matter here.
          </p>
          <p>
            First, the failure mode is explicit. A pool can be exhausted, and when it is, you get
            <code>nullptr</code>. Nothing crashes. You can drop the oldest projectile, deny a spawn, or log
            and move on. The allocator does not get to invent behavior.
          </p>
          <p>
            Second, fragmentation is structurally impossible. Every slot is the same size, so every free slot
            is interchangeable with every other.
          </p>
          <p>
            From a design point of view, pools force a habit that pays off later: you choose a budget per class
            of object. "This build supports 10k live projectiles and 2k minions," not "we hope no one brings
            the machine to its knees with too many spawns."
          </p>

          <h3>Free-list allocator: the general heap, but predictable</h3>
          <p>
            The third layer is the free-list allocator that backs the "1.5&nbsp;GB general heap" bucket.
          </p>
          <p>
            This is for everything that does not fit the previous two patterns:
          </p>
          <ul>
            <li>Meshes, textures, and other content.</li>
            <li>Persistent game systems.</li>
            <li>Scripting VMs, UI trees, and long-lived gameplay data.</li>
          </ul>
          <p>
            Internally, the allocator uses:
          </p>
          <ul>
            <li>A set of size classes (e.g., 8&nbsp;B, 16&nbsp;B, 32&nbsp;B, … up to 128&nbsp;KB), each with its own free-list.</li>
            <li>A fallback path for large blocks beyond that range.</li>
            <li>Coalescing of neighbors on free, to reduce long-term fragmentation.</li>
          </ul>
          <p>
            The performance goal is not "never slow down" but "behave consistently":
          </p>
          <ul>
            <li>Small allocations usually come from a size-class free-list in O(1).</li>
            <li>Deallocation returns blocks to the appropriate bin and may link neighbors.</li>
            <li>Large allocations do more bookkeeping, but they are rare and visible.</li>
          </ul>
          <p>
            The free-list allocator is allowed to be more complex because it is not in the tightest loops.
            Whenever something shows up hot in a profile, the first fix is not to micro-optimize the heap;
            it is to move that site onto a frame or pool allocator.
          </p>

          <h3>Tracy integration: making memory visible</h3>
          <p>
            A memory system without telemetry is like a fuzzer without coverage: you can tell that
            <em>something</em> is happening, but you cannot tell whether it is progress.
          </p>
          <p>
            We wire allocators into Tracy via <code>tracy_memory.h</code>. The contract is:
          </p>
          <ul>
            <li>Allocators call <code>PHALNX_TRACY_ALLOC(ptr, size, pool_name)</code> when they hand out memory.</li>
            <li>They call <code>PHALNX_TRACY_FREE(ptr, pool_name)</code> when they take it back.</li>
            <li>The macros are no-ops when Tracy is disabled.</li>
          </ul>
          <p>
            On top of these primitives there are small helpers:
          </p>
          <ul>
            <li><code>TrackBatchAllocation</code> / <code>UntrackBatchAllocation</code> for "one big buffer, many internal users" patterns like the frame allocator.</li>
            <li><code>TrackedAllocation&lt;T, Allocator&gt;</code> as an RAII wrapper that allocates in the constructor, frees in the destructor, and emits the appropriate Tracy events around both.</li>
          </ul>
          <p>
            This is enough to make each allocator show up as a named pool in Tracy's memory view.
          </p>
          <p>
            The other half is budgets.
          </p>
          <p>
            <code>MemoryBudget</code> encodes the 16&nbsp;MB / 512&nbsp;MB / 1.5&nbsp;GB / 2&nbsp;GB split and
            the warning/critical thresholds (80% and 95%). <code>MemoryBudgetTracker</code> keeps running
            tallies:
          </p>
          <ul>
            <li>How many bytes the frame allocator is using.</li>
            <li>How many bytes the pools are using, in aggregate.</li>
            <li>How many bytes the general heap is using.</li>
          </ul>
          <p>
            Each frame, it pushes plots to Tracy with names like:
          </p>
          <ul>
            <li><code>Memory/Frame</code></li>
            <li><code>Memory/Pool</code></li>
            <li><code>Memory/Heap</code></li>
            <li><code>Memory/Total</code></li>
          </ul>
          <p>
            This gives you the simplest possible kind of saturation signal: if <code>Memory/Pool</code> creeps
            up over the course of a match and never comes back down, there is a leak in or near a pool. If
            <code>Memory/Frame</code> spikes transiently alongside a physics spike, you know exactly where to
            look.
          </p>

          <h3>How it is used in real systems</h3>
          <p>
            This is all fairly abstract. It helps to look at a few concrete integration patterns that are
            already in motion.
          </p>

          <h4>Physics: per-thread scratch with no contention</h4>
          <p>
            Physics is a textbook source of allocator pain. It wants to allocate lots of small, short-lived
            structures (contacts, joints, queries) across multiple threads, every frame.
          </p>
          <p>
            In PHALANX, each physics worker thread receives:
          </p>
          <ul>
            <li>Its own frame allocator instance for per-step scratch.</li>
            <li>A set of dedicated pools for recurring structures like <code>ContactPoint</code> and <code>RaycastHit</code>.</li>
          </ul>
          <p>
            The "real" heap does not see any of those allocations.
          </p>
          <p>
            The end result is that physics does not fight with gameplay or rendering over the general allocator,
            and lock contention in the allocator simply does not exist. All of the thread safety work happens
            at the physics and ECS layer, not inside <code>malloc</code>.
          </p>

          <h4>Stagger: pre-allocating the worst-case spikes</h4>
          <p>
            Stagger and knockback effects are another pattern: nothing happens for a while, and then a lot
            happens at once.
          </p>
          <p>
            The plan there is:
          </p>
          <ul>
            <li>Size a dedicated pool for the worst realistic case: all mechs staggered at once, plus safety margin.</li>
            <li>Use it as a double-buffered resource: one half for this frame, one half for the next.</li>
            <li>Reset the inactive half in O(1) at the frame boundary, instead of individually freeing events.</li>
          </ul>
          <p>
            The reason to do this is not subtle performance wins; it is avoiding a very specific failure mode:
            "rare but huge bursts of allocations that fragment the heap and randomly blow the frame budget."
          </p>

          <h4>ECS and memory lining up</h4>
          <p>
            The ECS blog laid out targets like "20k-entity pass in &lt;1&nbsp;ms" and
            "<code>AddComponent</code> under 500&nbsp;ns".
          </p>
          <p>
            The memory system is designed to support those:
          </p>
          <ul>
            <li>Archetype tables and component arrays live in pools, not the general heap.</li>
            <li>Transient ECS views or staging structures use the frame allocator.</li>
            <li>Long-lived metadata (component registries, introspection, tooling) sits on the free-list heap.</li>
          </ul>
          <p>
            This keeps the fast ECS paths away from the allocator machinery that has to handle arbitrary sizes
            and try to fight fragmentation.
          </p>

          <h3>Rules we actually rely on</h3>
          <p>
            Most of the interesting behavior does not come from clever data structures; it comes from boring
            rules that are obeyed consistently.
          </p>
          <p>
            A few of them:
          </p>
          <p>
            <strong>1. Do not throw from allocators.</strong>
            All error paths return <code>nullptr</code>. If a pool is exhausted or a budget is exceeded, the
            allocator reports the fact; it does not invent a recovery story.
          </p>
          <p>
            <strong>2. Do not keep frame allocations past <code>Reset()</code>.</strong>
            Any API that takes a frame allocator makes that explicit. The documentation points out that storing
            those pointers is undefined behavior. This matches how things like <code>std::string_view</code>
            are documented: the lifetime is on you.
          </p>
          <p>
            <strong>3. Pool exhaustion is a gameplay problem.</strong>
            When a pool is full, something in design has to give: fewer projectiles, capped minion waves, or
            degraded visuals. This is intentional; it prevents silent heap growth under load.
          </p>
          <p>
            <strong>4. Alignment is explicit.</strong>
            Every allocator API takes an alignment parameter. Passing a non-power-of-two is a debug-time error.
            This prevents "some platforms break when we turn on AVX" class bugs from hiding inside the allocator.
          </p>
          <p>
            <strong>5. Telemetry is not optional in development.</strong>
            The Tracy hooks are compiled out in shipping builds, but development and test builds are expected
            to run with them on. Memory without graphs is just vibes.
          </p>

          <h3>Where it goes next</h3>
          <p>
            Up to this point, the memory system gives PHALANX a few important properties:
          </p>
          <ul>
            <li>A written-down budget and a way to see when reality deviates from it.</li>
            <li>Clear separation between frame, pool, and heap-style allocations.</li>
            <li>Allocators that are fast because they are allowed to be strict about how they are used.</li>
            <li>Instrumentation that makes misbehavior visible instead of surprising.</li>
          </ul>
          <p>
            From here, the roadmap looks familiar if you read the ECS post:
          </p>
          <ul>
            <li>Tie specific ECS archetypes and systems to explicit pools, so "too many projectiles" is visible as "ProjectilePool at 95% capacity."</li>
            <li>Integrate asset streaming with the free-list allocator and the budget tracker, so content can see which bundles actually cost how much memory.</li>
            <li>Build in-engine tooling that overlays memory graphs next to ECS iteration time and system timings, so cross-cutting regressions do not require multiple tools to diagnose.</li>
          </ul>
          <p>
            The intent is not to invent a novel allocator. It is to make memory as boring and as observable as
            possible, so time can be spent on the parts of PHALANX that are supposed to be interesting: mechs,
            maps, projectiles, and the fact that there are tens of thousands of entities moving every frame
            without the allocator becoming the bottleneck.
          </p>
        </div>
      </article>
    </section>
  </div>
</main>

<footer>
  <div class="container">
    <p>
      <a href="https://github.com/N0V4-Labs" style="color: var(--link-color); text-decoration: none;">github</a>
      &nbsp;•&nbsp;
      <a href="about.html" style="color: var(--link-color); text-decoration: none;">about</a>
    </p>
  </div>
</footer>
</body>
</html>
